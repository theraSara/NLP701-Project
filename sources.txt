Some GitHub and other code sources:

1. BertViz: Visualize Attention in NLP Models: https://github.com/jessevig/bertviz
2. Captum: https://captum.ai/tutorials/Bert_SQUAD_Interpret
3. SHAP (SHapley Additive exPlanations): https://github.com/shap/shap
4. Lime: Explaining the predictions of any machine learning classifier: https://github.com/marcotcr/lime
https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html


|          Phase         |  Timeline  |      Status    |         Deliverable         |
|------------------------|------------|----------------|-----------------------------|
| Literature Review      |      -     |    Complete    | Comparison table of methods |
| Model Training         | Week 7-8   |    Complete    | 6 trained models (3Ã—2)      |
| Data Sampling          | Week 8     |    Complete    | 200 sampled examples        |
| Midterm Report         | Week 9     |    Complete    | Progress summary            |
| Explanation Generation | Week 9     |    Complete    | ~2,400 explanations         |
| Evaluation Metrics     | Week 10-11 |    Complete    | Faithfulness & Stability    |
| LLM-as-Judge           | Week 11-12 |    Complete    | Qualitative ratings         |
| Analysis & Writing     | Week 13    |    Complete    | Final report                |
| Final Presentation     | Week 14    |    Complete    | Slides & presentation       |


PHASE 1: Setup and Baseline
1. Set the environment --> DONE

2. Data loading and exploration --> DONE
    - load SST-2 and IMDB using HuggingFace (HF) ``datasets``
    - create stratified samples (200 examples)
    - document dataset statistics

3. Model fine-tuning --> DONE
    - starting with DistilBERT on SST-2
    - use HF trainer API
    - track metrics: accuracy, F1, training time
    - save checkpoints

PHASE 2: Interpretability Methods
1. Attention Visualization --> DONE
    - extract attention weights from the models 
    - visualize using BertViz 
    - save attention matrices for evaluation

2. Integrated Gradients --> DONE
    - use Captum's ``LayerIntegratedGradients``
    - generate attrubutions for sampled examples

3. SHAP --> DONE
    - use ``shap.Explainer`` with transformer models

4. LIME --> DONE
    - create text explainer 
    - generate local explanations

PHASE 3: Evaluation 
1. Faithfulness
2. Stability
3. Efficiency

Work division:
Divide the work by dataset.
1. SST-2 dataset has 100 total samples
3 models
4 methods
So the total number of samples would be: 100*3*4=1200 explanations

2. IMDB dataset has 100 total samples
3 models
4 methods
So the total number of samples would be: 100*3*4=1200 explanations

This way each one of us will focus on one dataset and we can work in parallel instead of waiting for each others results.

